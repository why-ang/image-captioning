{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/why/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from random import shuffle, seed\n",
    "import string\n",
    "# non-standard dependencies:\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import skimage.io\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(imgs):\n",
    "    count_thr = 5\n",
    "\n",
    "  \n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "        for sent in img['sentences']:\n",
    "            for w in sent['tokens']:\n",
    "                counts[w] = counts.get(w, 0) + 1\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)     # 按词频排序\n",
    "    print('top words and their counts:')\n",
    "    print('\\n'.join(map(str,cw[:20])))      #打印前20个词频最高的词\n",
    "\n",
    "  # 统计词频分布\n",
    "    total_words = sum(counts.values())    #总单词数\n",
    "    print('total words:', total_words)     \n",
    "    bad_words = [w for w,n in counts.items() if n <= count_thr]   #统计词频小于5的单词\n",
    "    vocab = [w for w,n in counts.items() if n > count_thr]      #用词频大于5的单词构建词典\n",
    "    bad_count = sum(counts[w] for w in bad_words)                # 统计所有低频词出现的次数\n",
    "    \n",
    "    print('number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "    print('number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print('number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))\n",
    "\n",
    "  # 统计句子长度分布\n",
    "    sent_lengths = {}\n",
    "    for img in imgs:\n",
    "        for sent in img['sentences']:\n",
    "            txt = sent['tokens']\n",
    "            nw = len(txt)\n",
    "            sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n",
    "    max_len = max(sent_lengths.keys())\n",
    "    print('max length sentence in raw data: ', max_len)\n",
    "    print('sentence length distribution (count, number of words):')\n",
    "    sum_len = sum(sent_lengths.values())\n",
    "    for i in range(max_len+1):\n",
    "        print('%2d: %10d   %f%%' % (i, sent_lengths.get(i,0), sent_lengths.get(i,0)*100.0/sum_len))\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "  # 添加未知词  （词频<=5的词归为未知词UNK ）\n",
    "    if bad_count > 0:\n",
    "        print('inserting the special UNK token')\n",
    "        vocab.append('UNK') \n",
    "  \n",
    "    for img in imgs:\n",
    "        img['final_captions'] = []\n",
    "        for sent in img['sentences']:\n",
    "            txt = sent['tokens']\n",
    "            caption = [w if counts.get(w,0) > count_thr else 'UNK' for w in txt]\n",
    "            img['final_captions'].append(caption)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(imgs, wtoi):\n",
    "\n",
    "    max_length = 16\n",
    "    N = len(imgs)\n",
    "    M = sum(len(img['final_captions']) for img in imgs) # 计算总的句子数\n",
    "\n",
    "    label_arrays = []\n",
    "    label_start_ix = np.zeros(N, dtype='uint32') # note: these will be one-indexed\n",
    "    label_end_ix = np.zeros(N, dtype='uint32')\n",
    "    label_length = np.zeros(M, dtype='uint32')\n",
    "    caption_counter = 0\n",
    "    counter = 1\n",
    "    for i,img in enumerate(imgs):\n",
    "        n = len(img['final_captions'])\n",
    "        assert n > 0, 'error: some image has no captions'\n",
    "\n",
    "        Li = np.zeros((n, max_length), dtype='uint32')\n",
    "        for j,s in enumerate(img['final_captions']):\n",
    "            label_length[caption_counter] = min(max_length, len(s)) # 判断当前图片caption 长度是否  大于最长句子\n",
    "            caption_counter += 1\n",
    "            for k,w in enumerate(s):\n",
    "                if k < max_length:      #  超过长度的截断 \n",
    "                    Li[j,k] = wtoi[w]  \n",
    "\n",
    "        # note: word indices are 1-indexed, and captions are padded with zeros\n",
    "        label_arrays.append(Li)\n",
    "        label_start_ix[i] = counter    # 图像  第一个描述  标号\n",
    "        label_end_ix[i] = counter + n - 1   # 图像结尾描述标号\n",
    "\n",
    "        counter += n\n",
    "\n",
    "    L = np.concatenate(label_arrays, axis=0) # 所有caption 拼接起来   （图像数 * 5， max_len）\n",
    "    assert L.shape[0] == M, 'lengths don\\'t match? that\\'s weird'\n",
    "    assert np.all(label_length > 0), 'error: some caption had no words?'\n",
    "\n",
    "    print('encoded captions to array of size ', L.shape)\n",
    "    return L, label_start_ix, label_end_ix, label_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    imgs = json.load(open(input_json, 'r'))\n",
    "    imgs = imgs['images']    \n",
    "\n",
    "#     with open(\"/home/why/image-captioning-bottom-up-top-down/other_dataset/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\", 'r') as j:\n",
    "#         word_map = json.load(j)\n",
    "\n",
    "    with open(\"./data/top_1_topic_class.json\", 'r') as j:\n",
    "        image_topclass= json.load(j)\n",
    "\n",
    "    with open(\"./data/top_1_topic_word.json\", 'r') as j:\n",
    "        image_topword= json.load(j)\n",
    "\n",
    "  #############################################################################  \n",
    "    \n",
    "    seed(123) # make reproducible\n",
    "    vocab = build_vocab(imgs)\n",
    "    \n",
    "    \n",
    "    wtoi = {w:i+1 for i,w in enumerate(vocab)} # 词  -->  数字\n",
    "    itow = {i+1:w for i,w in enumerate(vocab)} # 数字  -->  词\n",
    "    \n",
    "    L, label_start_ix, label_end_ix, label_length = encode_captions(imgs, wtoi)\n",
    "    \n",
    "    N = len(imgs)\n",
    "    f_lb = h5py.File(output_h5+'_label.h5', \"w\")\n",
    "    f_lb.create_dataset(\"labels\", dtype='uint32', data=L)                         \n",
    "    f_lb.create_dataset(\"label_start_ix\", dtype='uint32', data=label_start_ix)\n",
    "    f_lb.create_dataset(\"label_end_ix\", dtype='uint32', data=label_end_ix)\n",
    "    f_lb.create_dataset(\"label_length\", dtype='uint32', data=label_length)\n",
    "    f_lb.close()\n",
    "\n",
    "    out = {}\n",
    "    out['ix_to_word'] = itow # encode the (1-indexed) vocab\n",
    "    out['images'] = []\n",
    "    for i,img in enumerate(imgs):\n",
    "        jimg = {}\n",
    "        jimg['split'] = img['split']\n",
    "        if 'filename' in img: jimg['file_path'] = os.path.join(img['filepath'], img['filename']) # copy it over, might need\n",
    "        if 'cocoid' in img: jimg['id'] = img['cocoid'] # copy over & mantain an id, if present (e.g. coco ids, useful)\n",
    "        \n",
    "        \n",
    "        image_id = img['filename'].split('_')[2]\n",
    "        image_id = int(image_id.lstrip(\"0\").split('.')[0])\n",
    "\n",
    "        top_class = image_topclass[str(image_id)]\n",
    "        word = image_topword[str(image_id)]\n",
    "        word = word.split(' ')\n",
    "        enc_w = [wtoi.get(word, word_map['<unk>']) for word in word]\n",
    "        jimg['topic_class'] = top_class\n",
    "        jimg['topic_word'] = enc_w\n",
    "        \n",
    "        \n",
    "        if images_root != '':\n",
    "            with Image.open(os.path.join(images_root, img['filepath'], img['filename'])) as _img:\n",
    "                jimg['width'], jimg['height'] = _img.size\n",
    "\n",
    "        out['images'].append(jimg)\n",
    "\n",
    "    json.dump(out, open(output_json, 'w'))\n",
    "    print('wrote ', output_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_root='/home/sdb1/why/COCO'\n",
    "input_json='/home/why/data/caption/dataset_coco.json'\n",
    "output_json ='/home/sdb1/why/self-critical/data/data.json'\n",
    "output_h5 ='/home/sdb1/why/self-critical/data/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_json ='/home/sdb1/why/self-critical/len_16/data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(output_json,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[261, 74, 99, 98, 75, 102, 350, 80, 835, 93, 2835, 89, 905, 347, 254, 213, 157, 1794, 1732, 111]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    if i['id'] == 177015:\n",
    "        print(i['topic_class'])\n",
    "        print(i['topic_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = data['ix_to_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [261, 74, 99, 98, 75, 102, 350, 80, 835, 93, 2835, 89, 905, 347, 254, 213, 157, 1794, 1732, 111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "computer\n",
      "desk\n",
      "sitting\n",
      "keyboard\n",
      "table\n",
      "mouse\n",
      "monitor\n",
      "office\n",
      "computers\n",
      "laptops\n",
      "using\n",
      "screen\n",
      "lap\n",
      "open\n",
      "wooden\n",
      "sits\n",
      "desktop\n",
      "monitors\n",
      "working\n"
     ]
    }
   ],
   "source": [
    "for i in t:\n",
    "    print(w[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words and their counts:\n",
      "(1019785, 'a')\n",
      "(224758, 'on')\n",
      "(212689, 'of')\n",
      "(206178, 'the')\n",
      "(191793, 'in')\n",
      "(161216, 'with')\n",
      "(146755, 'and')\n",
      "(102390, 'is')\n",
      "(75957, 'man')\n",
      "(71183, 'to')\n",
      "(55190, 'sitting')\n",
      "(51987, 'an')\n",
      "(50467, 'two')\n",
      "(44506, 'at')\n",
      "(44297, 'standing')\n",
      "(43707, 'people')\n",
      "(42776, 'are')\n",
      "(38867, 'next')\n",
      "(37898, 'white')\n",
      "(35372, 'woman')\n",
      "total words: 6454115\n",
      "number of bad words: 18443/27929 = 66.04%\n",
      "number of words in vocab would be 9486\n",
      "number of UNKs: 32382/6454115 = 0.50%\n",
      "max length sentence in raw data:  49\n",
      "sentence length distribution (count, number of words):\n",
      " 0:          0   0.000000%\n",
      " 1:          0   0.000000%\n",
      " 2:          0   0.000000%\n",
      " 3:          0   0.000000%\n",
      " 4:          0   0.000000%\n",
      " 5:          1   0.000162%\n",
      " 6:         14   0.002270%\n",
      " 7:       4851   0.786521%\n",
      " 8:     101387   16.438461%\n",
      " 9:     134531   21.812289%\n",
      "10:     132558   21.492395%\n",
      "11:      95206   15.436299%\n",
      "12:      60590   9.823807%\n",
      "13:      35233   5.712530%\n",
      "14:      20016   3.245310%\n",
      "15:      11476   1.860670%\n",
      "16:       6922   1.122304%\n",
      "17:       4313   0.699292%\n",
      "18:       2755   0.446684%\n",
      "19:       1913   0.310166%\n",
      "20:       1312   0.212722%\n",
      "21:        923   0.149651%\n",
      "22:        665   0.107820%\n",
      "23:        503   0.081554%\n",
      "24:        328   0.053181%\n",
      "25:        258   0.041831%\n",
      "26:        194   0.031454%\n",
      "27:        156   0.025293%\n",
      "28:         97   0.015727%\n",
      "29:         74   0.011998%\n",
      "30:         52   0.008431%\n",
      "31:         65   0.010539%\n",
      "32:         41   0.006648%\n",
      "33:         48   0.007783%\n",
      "34:         43   0.006972%\n",
      "35:         35   0.005675%\n",
      "36:         21   0.003405%\n",
      "37:         24   0.003891%\n",
      "38:         20   0.003243%\n",
      "39:         21   0.003405%\n",
      "40:         19   0.003081%\n",
      "41:         21   0.003405%\n",
      "42:         11   0.001783%\n",
      "43:         19   0.003081%\n",
      "44:         18   0.002918%\n",
      "45:         13   0.002108%\n",
      "46:          6   0.000973%\n",
      "47:          7   0.001135%\n",
      "48:          3   0.000486%\n",
      "49:          4   0.000649%\n",
      "inserting the special UNK token\n",
      "encoded captions to array of size  (616767, 16)\n",
      "wrote  /home/sdb1/why/self-critical/data/data.json\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b1d0e5878c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "aa = json.load(open(output_json,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii = aa['images'][55]['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': 'restval',\n",
       " 'file_path': 'val2014/COCO_val2014_000000191381.jpg',\n",
       " 'id': 191381,\n",
       " 'topic_class': 41,\n",
       " 'topic_word': [432,\n",
       "  219,\n",
       "  163,\n",
       "  46,\n",
       "  438,\n",
       "  454,\n",
       "  7,\n",
       "  482,\n",
       "  544,\n",
       "  123,\n",
       "  489,\n",
       "  480,\n",
       "  497,\n",
       "  98,\n",
       "  120,\n",
       "  218,\n",
       "  194,\n",
       "  457,\n",
       "  733,\n",
       "  527],\n",
       " 'width': 640,\n",
       " 'height': 480}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa['images'][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii = images_root+'/'+iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdb1/why/COCO/val2014/COCO_val2014_000000191381.jpg'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79ee62b4c0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "image = Image.open(iii)\n",
    "plt.imshow(image)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
