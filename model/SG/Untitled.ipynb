{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EvaluatorLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha: float = 0.5, beta: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, evaluator_outputs, generator_outputs, other_outputs):\n",
    "        generator_outputs = 1 - generator_outputs\n",
    "        other_outputs = 1 - other_outputs\n",
    "        temp = 1e-5\n",
    "        evaluator_outputs = evaluator_outputs.clamp(min=temp)\n",
    "        generator_outputs = generator_outputs.clamp(min=temp)\n",
    "        other_outputs = other_outputs.clamp(min=temp)\n",
    "        t1, t2, t3 = torch.log(evaluator_outputs), self.alpha * torch.log(generator_outputs), self.beta * torch.log(\n",
    "            other_outputs)\n",
    "        result = t1 + t2 + t3\n",
    "        result = -result\n",
    "        return result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data_folder = '/home/why/image-captioning-bottom-up-top-down/other_dataset'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "checkpoint = \"BEST_42checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\"\n",
    "\n",
    "def evaluator_main():\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "    checkpoint = \"BEST_42checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\"\n",
    "    \n",
    "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "    with open(word_map_file, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    generator = checkpoint['decoder']\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    \n",
    "    evaluator = Evaluator( embed_dim = 1024, decoder_dim = 1024, vocab_size = len(word_map),\n",
    "                           features_dim=2048, dropout=0.5,max_len = 20)\n",
    "    evaluator = evaluator.to(device)\n",
    "    \n",
    "    batch_size = 128\n",
    "    dataloader = torch.utils.data.DataLoader(CaptionDataset(data_folder, data_name, 'TRAIN'),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    criterion = EvaluatorLoss().cuda()\n",
    "    optimizer = optim.Adam(evaluator.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "#######################################################################################################    \n",
    "    \n",
    "    epoch = 10\n",
    "    print(f\"number of batches = {len(dataloader) // batch_size}\")\n",
    "    print(\"Begin Training\")\n",
    "    for epoch in range(epochs):\n",
    "        cap_out = 0\n",
    "        gen_out = 0\n",
    "        for i, (imgs, caps, _,oth_caps,_ ) in enumerate(dataloader):\n",
    "            start = time.time()\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            caps = generator.embedding(caps)  # (batch_size, max_caption_length, embed_dim)\n",
    "            oth_caps = oth_caps.to(device)\n",
    "            oth_caps = generator.embedding(oth_caps) \n",
    "            \n",
    "            generator_outputs = generator.sample(imgs)\n",
    "            generator_outputs = tran(generator_outputs)\n",
    "            generator_outputs = generator.embedding(generator_outputs) \n",
    "\n",
    "            evaluator_outputs = evaluator(imgs, caps)     #  鉴别器 鉴别  image 正确 caption  \n",
    "            generator_outputs = evaluator(imgs, generator_outputs)  #  image  生成的虚假caption\n",
    "            other_outputs = evaluator(imgs, oth_caps)  # image  其他图像的caption\n",
    "            cap_out = cap_out+evaluator_outputs.sum()\n",
    "            gen_out = gen_out+generator_outputs.sum()\n",
    "            \n",
    "            loss = criterion(evaluator_outputs, generator_outputs, other_outputs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            end = time.time()\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Batch Time {batch_time:.3f}\\t'\n",
    "                      'Loss {loss:.4f} '.format(epoch, i, len(dataloader),\n",
    "                                                batch_time=end - start,loss=loss))\n",
    "            start = end\n",
    "        \n",
    "        evaluator.save()\n",
    "        cap_ave = cap_out/113287\n",
    "        gen_ave = gen_out/113287\n",
    "        print(\"cap_ave:------\",cap_ave)\n",
    "        print(\"gen_ave:------\",gen_ave)\n",
    "        if cap_ave>0.5 and gen_ave<0.5:\n",
    "            break\n",
    "        print(f\"Epoch = {epoch + 1}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/why/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import traceback\n",
    "\n",
    "import opts\n",
    "import models\n",
    "from dataloader import *\n",
    "import skimage.io\n",
    "import eval_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6af78dacd512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.setup(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(opt.model))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "crit = utils.LanguageModelCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    # Create the Data Loader instance\n",
    "    if len(opt.image_folder) == 0:\n",
    "        loader = DataLoader(opt)\n",
    "    else:\n",
    "        loader = DataLoaderRaw({'folder_path': opt.image_folder, \n",
    "                                'coco_json': opt.coco_json,\n",
    "                                'batch_size': opt.batch_size,\n",
    "                                'cnn_model': opt.cnn_model})\n",
    "    \n",
    "    beam_size = eval_kwargs.get('beam_size', 1)\n",
    "    # Make sure in the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    loader.reset_iterator(split)\n",
    "    n = 0\n",
    "    while True:\n",
    "        data = loader.get_batch('train')\n",
    "        n = n + loader.batch_size\n",
    "\n",
    "        if data.get('labels', None) is not None:\n",
    "            # forward the model to get loss\n",
    "            tmp = [data['fc_feats'], data['att_feats'], data['labels'], data['masks'], data['att_masks'],data['topic_word'],data['other_caption']]\n",
    "            tmp = [_.cuda() if _ is not None else _ for _ in tmp]\n",
    "            fc_feats, att_feats, labels, masks, att_masks, topic_word,other_caption= tmp\n",
    "\n",
    "        # forward the model to also get generated samples for each image\n",
    "        # Only leave one feature for each image, in case duplicate sample\n",
    "        tmp = [data['fc_feats'][np.arange(loader.batch_size) * loader.seq_per_img], \n",
    "            data['att_feats'][np.arange(loader.batch_size) * loader.seq_per_img],\n",
    "            data['topic_word'][np.arange(loader.batch_size) * loader.seq_per_img],\n",
    "            data['other_caption'][np.arange(loader.batch_size) * loader.seq_per_img],\n",
    "            data['att_masks'][np.arange(loader.batch_size) * loader.seq_per_img] if data['att_masks'] is not None else None]\n",
    "        tmp = [_.cuda() if _ is not None else _ for _ in tmp]\n",
    "        fc_feats, att_feats, topic_word, other_caption,att_masks = tmp\n",
    "      \n",
    "        # forward the model to also get generated samples for each image\n",
    "        with torch.no_grad():\n",
    "            seq = model(fc_feats, att_feats, topic_word, att_masks, opt=eval_kwargs, mode='sample')[0].data\n",
    "\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_summary_value(writer, key, value, iteration):\n",
    "    if writer:\n",
    "        writer.add_scalar(key, value, iteration)\n",
    "\n",
    "def train(opt):\n",
    "    opt.use_fc, opt.use_att = utils.if_use_feat(opt.caption_model)\n",
    "    if opt.use_box: opt.att_feat_size = opt.att_feat_size + 5\n",
    "\n",
    "    loader = DataLoader(opt)\n",
    "    opt.vocab_size = loader.vocab_size\n",
    "    opt.seq_length = loader.seq_length\n",
    "\n",
    "    tb_summary_writer = tb and tb.SummaryWriter(opt.checkpoint_path)\n",
    "\n",
    "    infos = {}\n",
    "    histories = {}\n",
    "    with open(os.path.join(opt.start_from, 'infos_'+opt.id+'.pkl'), 'rb') as f:\n",
    "        infos = utils.pickle_load(f)\n",
    "        saved_model_opt = infos['opt']\n",
    "        need_be_same=[\"caption_model\", \"rnn_type\", \"rnn_size\", \"num_layers\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_opt)[checkme] == vars(opt)[checkme], \"Command line argument and saved model disagree on '%s' \" % checkme\n",
    "\n",
    "    if os.path.isfile(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')):\n",
    "        with open(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl'), 'rb') as f:\n",
    "            histories = utils.pickle_load(f)\n",
    "    infos['opt'] = opt\n",
    "\n",
    "    iteration = infos.get('iter', 0)\n",
    "    epoch = infos.get('epoch', 0)\n",
    "\n",
    "    val_result_history = histories.get('val_result_history', {})\n",
    "    loss_history = histories.get('loss_history', {})\n",
    "    lr_history = histories.get('lr_history', {})\n",
    "    ss_prob_history = histories.get('ss_prob_history', {})\n",
    "\n",
    "    loader.iterators = infos.get('iterators', loader.iterators)\n",
    "    loader.split_ix = infos.get('split_ix', loader.split_ix)\n",
    "    if opt.load_best_score == 1:\n",
    "        best_val_score = infos.get('best_val_score', None)\n",
    "\n",
    "###################################################################################################        \n",
    "\n",
    "    model = models.setup(opt).cuda()\n",
    "    lw_model = LossWrapper(model, opt)\n",
    "    dp_lw_model = torch.nn.DataParallel(lw_model)\n",
    "    \n",
    "    epoch_done = True\n",
    "    # Assure in training mode\n",
    "    dp_lw_model.eval()\n",
    "\n",
    "    if opt.noamopt:\n",
    "        assert opt.caption_model == 'transformer', 'noamopt can only work with transformer'\n",
    "        optimizer = utils.get_std_opt(model, factor=opt.noamopt_factor, warmup=opt.noamopt_warmup)\n",
    "        optimizer._step = iteration\n",
    "    elif opt.reduce_on_plateau:\n",
    "        optimizer = utils.build_optimizer(model.parameters(), opt)\n",
    "        optimizer = utils.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "    else:\n",
    "        optimizer = utils.build_optimizer(model.parameters(), opt)\n",
    "    # Load the optimizer\n",
    "    if vars(opt).get('start_from', None) is not None and os.path.isfile(os.path.join(opt.start_from,\"optimizer.pth\")):\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, 'optimizer.pth')))\n",
    "\n",
    "    while True:\n",
    "        sc_flag = True\n",
    "        init_scorer(opt.cached_tokens)\n",
    "        start = time.time()\n",
    "        # Load data from train split (0)\n",
    "        data = loader.get_batch('train')\n",
    "        print('Read data:', time.time() - start)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        tmp = [data['fc_feats'], data['att_feats'], data['labels'], data['masks'], data['att_masks'],data['topic_word'],data['other_caption']]\n",
    "        tmp = [_ if _ is None else _.cuda() for _ in tmp]\n",
    "        fc_feats, att_feats, labels, masks, att_masks, topic_word,other_caption= tmp\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_out = dp_lw_model(fc_feats, att_feats, labels, masks,topic_word, att_masks, data['gts'], torch.arange(0, len(data['gts'])), sc_flag)\n",
    "        \n",
    "        \n",
    "        loss = model_out['loss'].mean()\n",
    "\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, opt.grad_clip)\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        print(\"iter {} (epoch {}), avg_reward = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(iteration, epoch, model_out['reward'].mean(), end - start))\n",
    "\n",
    "        # Update the iteration and epoch\n",
    "        iteration += 1\n",
    "        if data['bounds']['wrapped']:\n",
    "            epoch += 1\n",
    "            epoch_done = True\n",
    "\n",
    "        # Write the training loss summary\n",
    "        if (iteration % opt.losses_log_every == 0):\n",
    "            add_summary_value(tb_summary_writer, 'train_loss', train_loss, iteration)\n",
    "            if opt.noamopt:\n",
    "                opt.current_lr = optimizer.rate()\n",
    "            elif opt.reduce_on_plateau:\n",
    "                opt.current_lr = optimizer.current_lr\n",
    "            add_summary_value(tb_summary_writer, 'learning_rate', opt.current_lr, iteration)\n",
    "            add_summary_value(tb_summary_writer, 'scheduled_sampling_prob', model.ss_prob, iteration)\n",
    "            if sc_flag:\n",
    "                add_summary_value(tb_summary_writer, 'avg_reward', model_out['reward'].mean(), iteration)\n",
    "\n",
    "            loss_history[iteration] = train_loss if not sc_flag else model_out['reward'].mean()\n",
    "            lr_history[iteration] = opt.current_lr\n",
    "            ss_prob_history[iteration] = model.ss_prob\n",
    "\n",
    "        # update infos\n",
    "        infos['iter'] = iteration\n",
    "        infos['epoch'] = epoch\n",
    "        infos['iterators'] = loader.iterators\n",
    "        infos['split_ix'] = loader.split_ix\n",
    "\n",
    "        \n",
    "###################################################################################################################      \n",
    "        \n",
    "        # make evaluation on validation set, and save model\n",
    "        eval_kwargs = {'split': 'val',\n",
    "                        'dataset': opt.input_json}\n",
    "        eval_kwargs.update(vars(opt))\n",
    "        val_loss, predictions, lang_stats = eval_utils.eval_split(\n",
    "            dp_model, lw_model.crit, loader, eval_kwargs)\n",
    "        \n",
    " \n",
    "    # Stop if reaching max epochs\n",
    "    if epoch >= opt.max_epochs and opt.max_epochs != -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.Tensor(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0877e+11],\n",
       "        [4.5803e-41],\n",
       "        [9.4711e+03],\n",
       "        [3.0638e-41],\n",
       "        [4.4842e-44]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = aa.repeat(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0877e+11, 1.0877e+11, 1.0877e+11, 1.0877e+11, 1.0877e+11],\n",
       "        [4.5803e-41, 4.5803e-41, 4.5803e-41, 4.5803e-41, 4.5803e-41],\n",
       "        [9.4711e+03, 9.4711e+03, 9.4711e+03, 9.4711e+03, 9.4711e+03],\n",
       "        [3.0638e-41, 3.0638e-41, 3.0638e-41, 3.0638e-41, 3.0638e-41],\n",
       "        [4.4842e-44, 4.4842e-44, 4.4842e-44, 4.4842e-44, 4.4842e-44]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = bb.view(25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0877e+11],\n",
       "        [1.0877e+11],\n",
       "        [1.0877e+11],\n",
       "        [1.0877e+11],\n",
       "        [1.0877e+11],\n",
       "        [4.5803e-41],\n",
       "        [4.5803e-41],\n",
       "        [4.5803e-41],\n",
       "        [4.5803e-41],\n",
       "        [4.5803e-41],\n",
       "        [9.4711e+03],\n",
       "        [9.4711e+03],\n",
       "        [9.4711e+03],\n",
       "        [9.4711e+03],\n",
       "        [9.4711e+03],\n",
       "        [3.0638e-41],\n",
       "        [3.0638e-41],\n",
       "        [3.0638e-41],\n",
       "        [3.0638e-41],\n",
       "        [3.0638e-41],\n",
       "        [4.4842e-44],\n",
       "        [4.4842e-44],\n",
       "        [4.4842e-44],\n",
       "        [4.4842e-44],\n",
       "        [4.4842e-44]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
