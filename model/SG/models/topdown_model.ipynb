{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaptionModel, self).__init__()\n",
    "\n",
    "    # implements beam search\n",
    "    # calls beam_step and returns the final set of beams\n",
    "    # augments log-probabilities with diversity terms when number of groups > 1\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        mode = kwargs.get('mode', 'forward')\n",
    "        if 'mode' in kwargs:\n",
    "            del kwargs['mode']\n",
    "        return getattr(self, '_'+mode)(*args, **kwargs)\n",
    "\n",
    "    def beam_search(self, init_state, init_logprobs, *args, **kwargs):\n",
    "\n",
    "        # function computes the similarity score to be augmented\n",
    "        def add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash):\n",
    "            local_time = t - divm\n",
    "            unaug_logprobsf = logprobsf.clone()\n",
    "            for prev_choice in range(divm):\n",
    "                prev_decisions = beam_seq_table[prev_choice][local_time]\n",
    "                for sub_beam in range(bdash):\n",
    "                    for prev_labels in range(bdash):\n",
    "                        logprobsf[sub_beam][prev_decisions[prev_labels]] = logprobsf[sub_beam][prev_decisions[prev_labels]] - diversity_lambda\n",
    "            return unaug_logprobsf\n",
    "\n",
    "        # does one step of classical beam search\n",
    "\n",
    "        def beam_step(logprobsf, unaug_logprobsf, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n",
    "            #INPUTS:\n",
    "            #logprobsf: probabilities augmented after diversity\n",
    "            #beam_size: obvious\n",
    "            #t        : time instant\n",
    "            #beam_seq : tensor contanining the beams\n",
    "            #beam_seq_logprobs: tensor contanining the beam logprobs\n",
    "            #beam_logprobs_sum: tensor contanining joint logprobs\n",
    "            #OUPUTS:\n",
    "            #beam_seq : tensor containing the word indices of the decoded captions\n",
    "            #beam_seq_logprobs : log-probability of each decision made, same size as beam_seq\n",
    "            #beam_logprobs_sum : joint log-probability of each beam\n",
    "\n",
    "            ys,ix = torch.sort(logprobsf,1,True)\n",
    "            candidates = []\n",
    "            cols = min(beam_size, ys.size(1))\n",
    "            rows = beam_size\n",
    "            if t == 0:\n",
    "                rows = 1\n",
    "            for c in range(cols): # for each column (word, essentially)\n",
    "                for q in range(rows): # for each beam expansion\n",
    "                    #compute logprob of expanding beam q with word in (sorted) position c\n",
    "                    local_logprob = ys[q,c].item()\n",
    "                    candidate_logprob = beam_logprobs_sum[q] + local_logprob\n",
    "                    local_unaug_logprob = unaug_logprobsf[q,ix[q,c]]\n",
    "                    candidates.append({'c':ix[q,c], 'q':q, 'p':candidate_logprob, 'r':local_unaug_logprob})\n",
    "            candidates = sorted(candidates,  key=lambda x: -x['p'])\n",
    "            \n",
    "            new_state = [_.clone() for _ in state]\n",
    "            #beam_seq_prev, beam_seq_logprobs_prev\n",
    "            if t >= 1:\n",
    "            #we''ll need these as reference when we fork beams around\n",
    "                beam_seq_prev = beam_seq[:t].clone()\n",
    "                beam_seq_logprobs_prev = beam_seq_logprobs[:t].clone()\n",
    "            for vix in range(beam_size):\n",
    "                v = candidates[vix]\n",
    "                #fork beam index q into index vix\n",
    "                if t >= 1:\n",
    "                    beam_seq[:t, vix] = beam_seq_prev[:, v['q']]\n",
    "                    beam_seq_logprobs[:t, vix] = beam_seq_logprobs_prev[:, v['q']]\n",
    "                #rearrange recurrent states\n",
    "                for state_ix in range(len(new_state)):\n",
    "                #  copy over state in previous beam q to new beam at vix\n",
    "                    new_state[state_ix][:, vix] = state[state_ix][:, v['q']] # dimension one is time step\n",
    "                #append new end terminal at the end of this beam\n",
    "                beam_seq[t, vix] = v['c'] # c'th word is the continuation\n",
    "                beam_seq_logprobs[t, vix] = v['r'] # the raw logprob here\n",
    "                beam_logprobs_sum[vix] = v['p'] # the new (sum) logprob along this beam\n",
    "            state = new_state\n",
    "            return beam_seq,beam_seq_logprobs,beam_logprobs_sum,state,candidates\n",
    "\n",
    "        # Start diverse_beam_search\n",
    "        opt = kwargs['opt']\n",
    "        beam_size = opt.get('beam_size', 10)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        diversity_lambda = opt.get('diversity_lambda', 0.5)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        max_ppl = opt.get('max_ppl', 0)\n",
    "        length_penalty = utils.penalty_builder(opt.get('length_penalty', ''))\n",
    "        bdash = beam_size // group_size # beam per group\n",
    "\n",
    "        # INITIALIZATIONS\n",
    "        beam_seq_table = [torch.LongTensor(self.seq_length, bdash).zero_() for _ in range(group_size)]\n",
    "        beam_seq_logprobs_table = [torch.FloatTensor(self.seq_length, bdash).zero_() for _ in range(group_size)]\n",
    "        beam_logprobs_sum_table = [torch.zeros(bdash) for _ in range(group_size)]\n",
    "\n",
    "        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n",
    "        done_beams_table = [[] for _ in range(group_size)]\n",
    "        state_table = [list(torch.unbind(_)) for _ in torch.stack(init_state).chunk(group_size, 2)]\n",
    "        logprobs_table = list(init_logprobs.chunk(group_size, 0))\n",
    "        # END INIT\n",
    "\n",
    "        # Chunk elements in the args\n",
    "        args = list(args)\n",
    "        args = [_.chunk(group_size) if _ is not None else [None]*group_size for _ in args]\n",
    "        args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n",
    "\n",
    "        for t in range(self.seq_length + group_size - 1):\n",
    "            for divm in range(group_size): \n",
    "                if t >= divm and t <= self.seq_length + divm - 1:\n",
    "                    # add diversity\n",
    "                    logprobsf = logprobs_table[divm].data.float()\n",
    "                    # suppress previous word\n",
    "                    if decoding_constraint and t-divm > 0:\n",
    "                        logprobsf.scatter_(1, beam_seq_table[divm][t-divm-1].unsqueeze(1).cuda(), float('-inf'))\n",
    "                    # suppress UNK tokens in the decoding\n",
    "                    logprobsf[:,logprobsf.size(1)-1] = logprobsf[:, logprobsf.size(1)-1] - 1000  \n",
    "                    # diversity is added here\n",
    "                    # the function directly modifies the logprobsf values and hence, we need to return\n",
    "                    # the unaugmented ones for sorting the candidates in the end. # for historical\n",
    "                    # reasons :-)\n",
    "                    unaug_logprobsf = add_diversity(beam_seq_table,logprobsf,t,divm,diversity_lambda,bdash)\n",
    "\n",
    "                    # infer new beams\n",
    "                    beam_seq_table[divm],\\\n",
    "                    beam_seq_logprobs_table[divm],\\\n",
    "                    beam_logprobs_sum_table[divm],\\\n",
    "                    state_table[divm],\\\n",
    "                    candidates_divm = beam_step(logprobsf,\n",
    "                                                unaug_logprobsf,\n",
    "                                                bdash,\n",
    "                                                t-divm,\n",
    "                                                beam_seq_table[divm],\n",
    "                                                beam_seq_logprobs_table[divm],\n",
    "                                                beam_logprobs_sum_table[divm],\n",
    "                                                state_table[divm])\n",
    "\n",
    "                    # if time's up... or if end token is reached then copy beams\n",
    "                    for vix in range(bdash):\n",
    "                        if beam_seq_table[divm][t-divm,vix] == 0 or t == self.seq_length + divm - 1:\n",
    "                            final_beam = {\n",
    "                                'seq': beam_seq_table[divm][:, vix].clone(), \n",
    "                                'logps': beam_seq_logprobs_table[divm][:, vix].clone(),\n",
    "                                'unaug_p': beam_seq_logprobs_table[divm][:, vix].sum().item(),\n",
    "                                'p': beam_logprobs_sum_table[divm][vix].item()\n",
    "                            }\n",
    "                            final_beam['p'] = length_penalty(t-divm+1, final_beam['p'])\n",
    "                            # if max_ppl:\n",
    "                            #     final_beam['p'] = final_beam['p'] / (t-divm+1)\n",
    "                            done_beams_table[divm].append(final_beam)\n",
    "                            # don't continue beams from finished sequences\n",
    "                            beam_logprobs_sum_table[divm][vix] = -1000\n",
    "\n",
    "                    # move the current group one step forward in time\n",
    "                    \n",
    "                    it = beam_seq_table[divm][t-divm]\n",
    "                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(args[divm] + [state_table[divm]]))\n",
    "\n",
    "        # all beams are sorted by their log-probabilities\n",
    "        done_beams_table = [sorted(done_beams_table[i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n",
    "        done_beams = reduce(lambda a,b:a+b, done_beams_table)\n",
    "        return done_beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pack_padded_sequence(input, lengths):\n",
    "    sorted_lengths, indices = torch.sort(lengths, descending=True)\n",
    "    tmp = pack_padded_sequence(input[indices], sorted_lengths, batch_first=True)\n",
    "    inv_ix = indices.clone()\n",
    "    inv_ix[indices] = torch.arange(0,len(indices)).type_as(inv_ix)\n",
    "    return tmp, inv_ix\n",
    "\n",
    "def pad_unsort_packed_sequence(input, inv_ix):\n",
    "    tmp, _ = pad_packed_sequence(input, batch_first=True)\n",
    "    tmp = tmp[inv_ix]\n",
    "    return tmp\n",
    "\n",
    "def pack_wrapper(module, att_feats, att_masks):\n",
    "    if att_masks is not None:\n",
    "        packed, inv_ix = sort_pack_padded_sequence(att_feats, att_masks.data.long().sum(1))\n",
    "        return pad_unsort_packed_sequence(PackedSequence(module(packed[0]), packed[1]), inv_ix)\n",
    "    else:\n",
    "        return module(att_feats)\n",
    "\n",
    "class AttModel(CaptionModel):\n",
    "    def __init__(self):\n",
    "        super(AttModel, self).__init__()\n",
    "        self.vocab_size = 9487\n",
    "        self.input_encoding_size = 1024\n",
    "        self.rnn_size = 1024\n",
    "        self.num_layers = 1\n",
    "        self.drop_prob_lm = 0.5\n",
    "        self.seq_length = 20\n",
    "        self.fc_feat_size = 2048\n",
    "        self.att_feat_size = 2048\n",
    "        self.att_hid_size = 1024\n",
    "        \n",
    "\n",
    "        self.use_bn = 0\n",
    "        self.ss_prob = 0.0 # Schedule sampling probability\n",
    "\n",
    "        self.embed = nn.Sequential(nn.Embedding(self.vocab_size + 1, self.input_encoding_size),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(self.drop_prob_lm))\n",
    "        self.fc_embed = nn.Sequential(nn.Linear(self.fc_feat_size, self.rnn_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(self.drop_prob_lm))\n",
    "        self.att_embed = nn.Sequential(*(\n",
    "                                    ((nn.BatchNorm1d(self.att_feat_size),) if self.use_bn else ())+\n",
    "                                    (nn.Linear(self.att_feat_size, self.rnn_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(self.drop_prob_lm))+\n",
    "                                    ((nn.BatchNorm1d(self.rnn_size),) if self.use_bn==2 else ())))\n",
    "\n",
    "        self.logit_layers = 1\n",
    "        if self.logit_layers == 1:\n",
    "            self.logit = nn.Linear(self.rnn_size, self.vocab_size + 1)\n",
    "        \n",
    "        self.ctx2att = nn.Linear(self.rnn_size, self.att_hid_size)\n",
    "        \n",
    "        self.mean_w2rnn = nn.Linear(self.input_encoding_size, self.rnn_size)\n",
    "        self.w2rnn = nn.Linear(self.input_encoding_size, self.rnn_size)\n",
    "        self.wrnn2att = nn.Linear(self.rnn_size, self.att_hid_size)\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers+1, bsz, self.rnn_size),\n",
    "                weight.new_zeros(self.num_layers+1, bsz, self.rnn_size))\n",
    "\n",
    "    def clip_att(self, att_feats, att_masks):\n",
    "        # Clip the length of att_masks and att_feats to the maximum length\n",
    "        if att_masks is not None:\n",
    "            max_len = att_masks.data.long().sum(1).max()\n",
    "            att_feats = att_feats[:, :max_len].contiguous()\n",
    "            att_masks = att_masks[:, :max_len].contiguous()\n",
    "        return att_feats, att_masks\n",
    "\n",
    "    def _prepare_feature(self, fc_feats, att_feats, all_topic_word, att_masks):\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "\n",
    "        # embed fc and att feats\n",
    "        fc_feats = self.fc_embed(fc_feats)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        # Project the attention feats first to reduce memory and computation comsumptions.\n",
    "        p_att_feats = self.ctx2att(att_feats)\n",
    "        \n",
    "        all_topic_word_mean = all_topic_word.mean(1)\n",
    "        mean_word = self.mean_w2rnn(all_topic_word_mean)\n",
    "        att_word = self.w2rnn(all_topic_word)\n",
    "        p_att_word = self.wrnn2att(att_word)\n",
    "        \n",
    "\n",
    "        return fc_feats, att_feats, p_att_feats, mean_word, att_word, p_att_word, att_masks\n",
    "    \n",
    "    \n",
    "    ########################################################################################################\n",
    "\n",
    "\n",
    "    def _forward(self, fc_feats, att_feats, seq, all_topic_word, att_masks=None):\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size)\n",
    "\n",
    "        outputs = fc_feats.new_zeros(batch_size, seq.size(1) - 1, self.vocab_size+1)\n",
    "        \n",
    "        all_topic_word = self.embed(all_topic_word)\n",
    "        \n",
    "        p_fc_feats, p_att_feats, pp_att_feats,p_mean_word, p_att_word, pp_att_word, p_att_masks = self._prepare_feature(fc_feats, att_feats, all_topic_word , att_masks)\n",
    "\n",
    "        for i in range(seq.size(1) - 1):\n",
    "            if self.training and i >= 1 and self.ss_prob > 0.0: # otherwiste no need to sample\n",
    "                sample_prob = fc_feats.new(batch_size).uniform_(0, 1)\n",
    "                sample_mask = sample_prob < self.ss_prob\n",
    "                if sample_mask.sum() == 0:\n",
    "                    it = seq[:, i].clone()\n",
    "                else:\n",
    "                    sample_ind = sample_mask.nonzero().view(-1)\n",
    "                    it = seq[:, i].data.clone()\n",
    "                    prob_prev = torch.exp(outputs[:, i-1].detach()) # fetch prev distribution: shape Nx(M+1)\n",
    "                    it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1).index_select(0, sample_ind))\n",
    "            else:\n",
    "                it = seq[:, i].clone()          \n",
    "            if i >= 1 and seq[:, i].sum() == 0:\n",
    "                break\n",
    "\n",
    "            output, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats,p_mean_word, p_att_word, pp_att_word , p_att_masks,  state)\n",
    "            outputs[:, i] = output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_logprobs_state(self, it, fc_feats, att_feats, p_att_feats,p_mean_word, p_att_word, pp_att_word, att_masks, state):\n",
    "        # 'it' contains a word index\n",
    "        xt = self.embed(it)\n",
    "\n",
    "        output, state = self.core(xt, fc_feats, att_feats, p_att_feats,p_mean_word, p_att_word, pp_att_word, state, att_masks)\n",
    "        logprobs = F.log_softmax(self.logit(output), dim=1)\n",
    "\n",
    "        return logprobs, state\n",
    "    \n",
    "################################################################################################################\n",
    "\n",
    "    def _sample_beam(self, fc_feats, att_feats,all_topic_word, att_masks=None):\n",
    "        beam_size = 5\n",
    "        batch_size = fc_feats.size(0)\n",
    "        all_topic_word = self.embed(all_topic_word)\n",
    "        \n",
    "        \n",
    "        p_fc_feats, p_att_feats, pp_att_feats,p_mean_word, p_att_word, pp_att_word, p_att_masks = self._prepare_feature(fc_feats, att_feats, all_topic_word , att_masks)\n",
    "\n",
    "        assert beam_size <= self.vocab_size + 1, 'lets assume this for now, otherwise this corner case causes a few headaches down the road. can be dealt with in future if needed'\n",
    "        seq = torch.LongTensor(self.seq_length, batch_size).zero_()\n",
    "        seqLogprobs = torch.FloatTensor(self.seq_length, batch_size)\n",
    "        # lets process every image independently for now, for simplicity\n",
    "\n",
    "        self.done_beams = [[] for _ in range(batch_size)]\n",
    "        for k in range(batch_size):\n",
    "            state = self.init_hidden(beam_size)\n",
    "            tmp_fc_feats = p_fc_feats[k:k+1].expand(beam_size, p_fc_feats.size(1))\n",
    "            tmp_att_feats = p_att_feats[k:k+1].expand(*((beam_size,)+p_att_feats.size()[1:])).contiguous()\n",
    "            tmp_p_att_feats = pp_att_feats[k:k+1].expand(*((beam_size,)+pp_att_feats.size()[1:])).contiguous()\n",
    "            \n",
    "            tmp_mean_word = p_mean_word[k:k+1].expand(beam_size, p_mean_word.size(1))\n",
    "            tmp_att_word = p_att_word[k:k+1].expand(*((beam_size,)+p_att_word.size()[1:])).contiguous()\n",
    "            tmp_p_att_word = pp_att_word[k:k+1].expand(*((beam_size,)+pp_att_word.size()[1:])).contiguous()\n",
    "            \n",
    "            \n",
    "            tmp_att_masks = p_att_masks[k:k+1].expand(*((beam_size,)+p_att_masks.size()[1:])).contiguous() if att_masks is not None else None\n",
    "\n",
    "            for t in range(1):\n",
    "                if t == 0: # input <bos>\n",
    "                    it = fc_feats.new_zeros([beam_size], dtype=torch.long)\n",
    "\n",
    "                logprobs, state = self.get_logprobs_state(it, tmp_fc_feats, tmp_att_feats, tmp_p_att_feats,tmp_mean_word, tmp_att_word, tmp_p_att_word, tmp_att_masks, state)\n",
    "\n",
    "            self.done_beams[k] = self.beam_search(state, logprobs, tmp_fc_feats, tmp_att_feats, tmp_p_att_feats, tmp_mean_word, tmp_att_word, tmp_p_att_word,tmp_att_masks)\n",
    "            seq[:, k] = self.done_beams[k][0]['seq'] # the first beam has highest cumulative score\n",
    "            seqLogprobs[:, k] = self.done_beams[k][0]['logps']\n",
    "        # return the samples and their log likelihoods\n",
    "        return seq.transpose(0, 1), seqLogprobs.transpose(0, 1)\n",
    "\n",
    "    def _sample(self, fc_feats, att_feats, all_topic_word, att_masks=None):\n",
    "\n",
    "        sample_max = 1\n",
    "        beam_size = 5\n",
    "        temperature = 1.0\n",
    "        decoding_constraint = 0\n",
    "        block_trigrams = 0\n",
    "        if beam_size > 1:\n",
    "            return self._sample_beam(fc_feats, att_feats, all_topic_word,att_masks)\n",
    "\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size)\n",
    "        all_topic_word = self.embed(all_topic_word)\n",
    "        \n",
    "        \n",
    "        p_fc_feats, p_att_feats, pp_att_feats,p_mean_word, p_att_word, pp_att_word, p_att_masks = self._prepare_feature(fc_feats, att_feats, all_topic_word , att_masks)\n",
    "\n",
    "        trigrams = [] # will be a list of batch_size dictionaries\n",
    "\n",
    "        seq = fc_feats.new_zeros((batch_size, self.seq_length), dtype=torch.long)\n",
    "        seqLogprobs = fc_feats.new_zeros(batch_size, self.seq_length)\n",
    "        for t in range(self.seq_length + 1):\n",
    "            if t == 0: # input <bos>\n",
    "                it = fc_feats.new_zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "            logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats,p_mean_word, p_att_word, pp_att_word, p_att_masks, state)\n",
    "            \n",
    "            if decoding_constraint and t > 0:\n",
    "                tmp = logprobs.new_zeros(logprobs.size())\n",
    "                tmp.scatter_(1, seq[:,t-1].data.unsqueeze(1), float('-inf'))\n",
    "                logprobs = logprobs + tmp\n",
    "\n",
    "            # Mess with trigrams\n",
    "            if block_trigrams and t >= 3:\n",
    "                # Store trigram generated at last step\n",
    "                prev_two_batch = seq[:,t-3:t-1]\n",
    "                for i in range(batch_size): # = seq.size(0)\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    current  = seq[i][t-1]\n",
    "                    if t == 3: # initialize\n",
    "                        trigrams.append({prev_two: [current]}) # {LongTensor: list containing 1 int}\n",
    "                    elif t > 3:\n",
    "                        if prev_two in trigrams[i]: # add to list\n",
    "                            trigrams[i][prev_two].append(current)\n",
    "                        else: # create list\n",
    "                            trigrams[i][prev_two] = [current]\n",
    "                # Block used trigrams at next step\n",
    "                prev_two_batch = seq[:,t-2:t]\n",
    "                mask = torch.zeros(logprobs.size(), requires_grad=False).cuda() # batch_size x vocab_size\n",
    "                for i in range(batch_size):\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    if prev_two in trigrams[i]:\n",
    "                        for j in trigrams[i][prev_two]:\n",
    "                            mask[i,j] += 1\n",
    "                # Apply mask to log probs\n",
    "                #logprobs = logprobs - (mask * 1e9)\n",
    "                alpha = 2.0 # = 4\n",
    "                logprobs = logprobs + (mask * -0.693 * alpha) # ln(1/2) * alpha (alpha -> infty works best)\n",
    "\n",
    "            # sample the next word\n",
    "            if t == self.seq_length: # skip if we achieve maximum length\n",
    "                break\n",
    "            if sample_max:\n",
    "                sampleLogprobs, it = torch.max(logprobs.data, 1)\n",
    "                it = it.view(-1).long()\n",
    "            else:\n",
    "                if temperature == 1.0:\n",
    "                    prob_prev = torch.exp(logprobs.data) # fetch prev distribution: shape Nx(M+1)\n",
    "                else:\n",
    "                    # scale logprobs by temperature\n",
    "                    prob_prev = torch.exp(torch.div(logprobs.data, temperature))\n",
    "                it = torch.multinomial(prob_prev, 1)\n",
    "                sampleLogprobs = logprobs.gather(1, it) # gather the logprobs at sampled positions\n",
    "                it = it.view(-1).long() # and flatten indices for downstream processing\n",
    "\n",
    "            # stop when all finished\n",
    "            if t == 0:\n",
    "                unfinished = it > 0\n",
    "            else:\n",
    "                unfinished = unfinished * (it > 0)\n",
    "            it = it * unfinished.type_as(it)\n",
    "            seq[:,t] = it\n",
    "            seqLogprobs[:,t] = sampleLogprobs.view(-1)\n",
    "            # quit loop if all sequences have finished\n",
    "            if unfinished.sum() == 0:\n",
    "                break\n",
    "\n",
    "        return seq, seqLogprobs\n",
    "\n",
    "\n",
    "    \n",
    "################################################################################################################\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.rnn_size = 1024\n",
    "        self.att_hid_size = 1024\n",
    "        self.h2att = nn.Linear(self.rnn_size, self.att_hid_size)\n",
    "        self.alpha_net = nn.Linear(self.att_hid_size, 1)\n",
    "\n",
    "    def forward(self, h, att_feats, p_att_feats, att_masks=None):\n",
    "        # The p_att_feats here is already projected\n",
    "        att_size = att_feats.numel() // att_feats.size(0) // att_feats.size(-1)\n",
    "        att = p_att_feats.view(-1, att_size, self.att_hid_size)\n",
    "        \n",
    "        att_h = self.h2att(h)                        # batch * att_hid_size\n",
    "        att_h = att_h.unsqueeze(1).expand_as(att)            # batch * att_size * att_hid_size\n",
    "        dot = att + att_h                                   # batch * att_size * att_hid_size\n",
    "        dot = F.tanh(dot)                                # batch * att_size * att_hid_size\n",
    "        dot = dot.view(-1, self.att_hid_size)               # (batch * att_size) * att_hid_size\n",
    "        dot = self.alpha_net(dot)                           # (batch * att_size) * 1\n",
    "        dot = dot.view(-1, att_size)                        # batch * att_size\n",
    "        \n",
    "        weight = F.softmax(dot, dim=1)                             # batch * att_size\n",
    "        if att_masks is not None:\n",
    "            weight = weight * att_masks.view(-1, att_size).float()\n",
    "            weight = weight / weight.sum(1, keepdim=True) # normalize to 1\n",
    "        att_feats_ = att_feats.view(-1, att_size, att_feats.size(-1)) # batch * att_size * att_feat_size\n",
    "        att_res = torch.bmm(weight.unsqueeze(1), att_feats_).squeeze(1) # batch * att_feat_size\n",
    "\n",
    "        return att_res\n",
    "\n",
    "class TopDownCore(nn.Module):\n",
    "    def __init__(self, use_maxout=False):\n",
    "        super(TopDownCore, self).__init__()\n",
    "        self.drop_prob_lm = 0.5\n",
    "        \n",
    "        \n",
    "        self.v_lstm = nn.LSTMCell(3072, 1024) # we, fc, h^2_t-1\n",
    "        self.w_lstm = nn.LSTMCell(3072, 1024) # we, fc, h^2_t-1\n",
    "        \n",
    "        self.lang_lstm = nn.LSTMCell(4096,1024) # h^1_t, \\hat v\n",
    "        self.v_attention = Attention()\n",
    "        self.w_attention = Attention()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.ff = nn.Linear(4096,1024)\n",
    "        \n",
    "\n",
    "    def forward(self, xt, fc_feats, att_feats, p_att_feats, p_mean_word, p_att_word, pp_att_word, state, att_masks=None):\n",
    "        \n",
    "        prev_h = state[0][-1]   # p_h_lang\n",
    "        \n",
    "        v_lstm_input = torch.cat([prev_h, fc_feats, xt], 1)\n",
    "        h_v, c_v = self.v_lstm(v_lstm_input, (state[0][0], state[1][0]))\n",
    "        v_att = self.v_attention(h_v, att_feats, p_att_feats, att_masks)\n",
    "                \n",
    "        w_lstm_input = torch.cat([prev_h, p_mean_word , xt], 1)\n",
    "        h_w, c_w = self.w_lstm(w_lstm_input, (state[0][1], state[1][1]))\n",
    "        w_att = self.w_attention(h_w, p_att_word, pp_att_word, att_masks)\n",
    "        \n",
    "        h = torch.cat([h_v,h_w],1)\n",
    "        gate = self.sigmoid(self.ff(torch.cat([v_att,w_att,h],1)))\n",
    "        v_att = gate*v_att\n",
    "        w_att = (1-gate)*w_att\n",
    "        \n",
    "        lang_lstm_input = torch.cat([v_att, w_att,h], 1)\n",
    "\n",
    "        h_lang, c_lang = self.lang_lstm(lang_lstm_input, (state[0][2], state[1][2]))\n",
    "\n",
    "        output = F.dropout(h_lang, self.drop_prob_lm, self.training)\n",
    "        state = (torch.stack([h_v,h_w, h_lang]), torch.stack([c_v,c_w,c_lang]))\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "class TopDownModel(AttModel):\n",
    "    def __init__(self):\n",
    "        super(TopDownModel, self).__init__()\n",
    "        self.num_layers = 2\n",
    "        self.core = TopDownCore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopDownModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/home/sdb1/why/self-critical/save/model-best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
